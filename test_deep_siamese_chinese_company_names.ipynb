{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_deep-siamese_chinese_company_names",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/DongminWu/MLpractice/blob/master/test_deep_siamese_chinese_company_names.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "yIBfYkgp95Xg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "from scipy.signal import triang\n",
        "\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xXZm30OPEpA1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Data Generating"
      ]
    },
    {
      "metadata": {
        "id": "L2cICGsSFx76",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_of_each_type = 40\n",
        "num_of_weekend = 13\n",
        "day_length = 1440\n",
        "\n",
        "test_prop = 0.2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7zivZHOcJg80",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm *.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CvHTghKH_aZI",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7ab99771-d0d8-4d5b-dea7-d2a0724d90c4"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3c5ca556-d419-4f2a-99b0-c59ea591d22c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3c5ca556-d419-4f2a-99b0-c59ea591d22c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving company.txt to company.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MYmAZuW0_icH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"company.txt\", header=None)[750:1000]\n",
        "# data = pd.read_csv(\"company_short.txt\", header=None)[:250]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZqX9d9yUAz9u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = shuffle(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a-YnXSGTGHtz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "3715e9d3-49a0-4541-e5db-d89ff012f7f1"
      },
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "all_samples = []\n",
        "max_len = 0\n",
        "\n",
        "for it in data.iloc[:].iterrows():\n",
        "  item = it[1][0]\n",
        "  list_item = list(item)\n",
        "  all_samples.append(list_item)\n",
        "  max_len = len(list_item) if max_len < len(list_item) else max_len\n",
        "  for e in list_item:\n",
        "    all_words.append(e)\n",
        "\n",
        "all_words = np.array(all_words)\n",
        "uni_words = np.unique(all_words)\n",
        "\n",
        "print('words:', uni_words)\n",
        "print('max length:', max_len)\n",
        "\n",
        "\n",
        "one_hot = np.eye(uni_words.shape[0])\n",
        "\n",
        "word_dict = {}\n",
        "for i, e in enumerate(uni_words):\n",
        "  word_dict[e] = i\n"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "words: ['(' ')' '《' '》' '一' '万' '三' '上' '下' '世' '业' '东' '丝' '中' '丰' '乐' '九' '书'\n",
            " '事' '二' '云' '交' '产' '人' '仁' '仑' '仓' '代' '仪' '任' '份' '企' '会' '传' '住' '佳'\n",
            " '供' '侨' '俊' '保' '信' '修' '倩' '健' '储' '光' '公' '关' '兴' '具' '农' '冠' '冶' '出'\n",
            " '分' '划' '列' '创' '制' '刷' '券' '力' '办' '加' '务' '动' '劳' '勘' '包' '化' '北' '医'\n",
            " '升' '华' '南' '博' '卢' '印' '原' '厦' '县' '友' '双' '发' '变' '口' '司' '合' '同' '向'\n",
            " '吴' '告' '员' '和' '咨' '品' '售' '商' '啤' '啸' '器' '团' '园' '国' '图' '土' '地' '场'\n",
            " '城' '培' '基' '塑' '境' '处' '备' '复' '夏' '外' '大' '天' '套' '威' '媒' '子' '字' '学'\n",
            " '安' '宗' '宝' '实' '客' '宾' '寿' '小' '就' '局' '屋' '展' '属' '山' '岛' '岩' '峰' '川'\n",
            " '工' '巴' '市' '师' '平' '广' '应' '店' '康' '建' '开' '弋' '强' '影' '待' '微' '德' '心'\n",
            " '志' '思' '总' '恒' '息' '戎' '成' '房' '所' '托' '技' '投' '报' '拓' '招' '振' '探' '控'\n",
            " '摄' '放' '政' '数' '料' '斯' '新' '方' '施' '旅' '旦' '时' '昂' '昌' '明' '易' '星' '昭'\n",
            " '智' '月' '有' '服' '木' '本' '术' '机' '杂' '材' '来' '杰' '松' '构' '林' '桥' '梅' '械'\n",
            " '检' '森' '楼' '模' '欣' '欧' '民' '气' '水' '永' '汇' '江' '汽' '沪' '油' '泥' '泰' '泽'\n",
            " '泾' '洋' '流' '测' '济' '浚' '浦' '海' '消' '润' '深' '清' '渔' '港' '湾' '源' '滩' '漕'\n",
            " '潢' '灵' '炼' '烟' '热' '然' '煊' '煤' '燃' '版' '物' '环' '理' '生' '用' '田' '由' '申'\n",
            " '电' '畅' '界' '盐' '监' '盘' '盛' '睦' '石' '矿' '码' '研' '社' '祁' '神' '祥' '票' '禄'\n",
            " '福' '科' '租' '稀' '程' '税' '究' '空' '立' '站' '第' '筑' '算' '管' '箱' '粮' '精' '系'\n",
            " '纯' '线' '经' '结' '绝' '统' '维' '综' '绿' '缆' '罐' '置' '美' '翔' '翼' '耀' '而' '耘'\n",
            " '联' '股' '能' '腐' '腾' '自' '至' '航' '舶' '船' '艺' '艾' '芯' '花' '苑' '英' '茂' '范'\n",
            " '草' '荣' '药' '萘' '营' '虹' '行' '装' '西' '规' '视' '解' '计' '认' '训' '讯' '设' '证'\n",
            " '诚' '询' '象' '财' '责' '货' '质' '贸' '赁' '资' '赛' '超' '越' '路' '蹊' '车' '转' '轮'\n",
            " '输' '达' '迎' '运' '近' '进' '远' '连' '送' '通' '造' '道' '邮' '部' '酒' '重' '野' '量'\n",
            " '金' '钢' '铁' '银' '铸' '销' '锦' '镜' '长' '门' '闵' '防' '阳' '阿' '际' '限' '院' '险'\n",
            " '隆' '隧' '集' '青' '静' '面' '韫' '顶' '饰' '馆' '马' '驶' '驾' '验' '骑' '高' '黄' '鼎'\n",
            " '齐']\n",
            "max length: 18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0cafV_CTHMkZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18f23a01-03e4-478a-8fef-7550883cc7a3"
      },
      "cell_type": "code",
      "source": [
        "samples = []\n",
        "for item in all_samples:\n",
        "  one_line = []\n",
        "  for e in item:\n",
        "    one_line.append(word_dict[e])\n",
        "  samples.append(np.array(one_line))\n",
        "\n",
        "  \n",
        "embedded_words = np.zeros([len(data), max_len, len(uni_words)])\n",
        "for i, item in enumerate(all_samples):\n",
        "  for j, character in enumerate(item):\n",
        "    embedded_words[i, j, word_dict[character]] = 1\n",
        "print(\"embedded data:\", embedded_words.shape)\n"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedded data: (250, 18, 433)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xDjzKbdMAGYY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "49715e91-5337-4015-fce3-7a66fdfc4885"
      },
      "cell_type": "code",
      "source": [
        "test_len = int(data.shape[0] * test_prop)\n",
        "\n",
        "test_data = embedded_words[:test_len,:,:]\n",
        "train_data = embedded_words[test_len:,:,:]\n",
        "\n",
        "print(\"test_data shape\", test_data.shape)\n",
        "print(\"train_data shape\", train_data.shape)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_data shape (50, 18, 433)\n",
            "train_data shape (200, 18, 433)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jnRQjwYk8aO1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train set\n",
        "\n",
        "\n",
        "Train set will consist two parts:\n",
        "\n",
        "1. Same name -> target = 1\n",
        "2. different name -> target = 0\n",
        "\n",
        "```\n",
        "\n",
        "for e in train_data\n",
        "    same = (e,e)\n",
        "    different = (e, train_data\\e)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "LKqtpzol_9in",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bf5f3331-7d35-4fb1-d063-9e76248c1d9f"
      },
      "cell_type": "code",
      "source": [
        "def pre(data):\n",
        "  data_idx = list(range(len(data)))\n",
        "\n",
        "  same_l_idx = data_idx\n",
        "  same_r_idx = data_idx\n",
        "  same_target = np.ones(len(data))\n",
        "\n",
        "  diff_l_idx = shuffle(data_idx)\n",
        "  diff_r_idx = shuffle(data_idx)\n",
        "  diff_target = np.zeros(len(data))\n",
        "\n",
        "\n",
        "  for i, pair in enumerate(zip(diff_l_idx, diff_r_idx)):\n",
        "    if pair[0] == pair[1]:\n",
        "      diff_target[i] = 1\n",
        "  \n",
        "  ret_l = same_l_idx + diff_l_idx\n",
        "  ret_r = same_r_idx + diff_r_idx\n",
        "  ret_target = np.concatenate([same_target , diff_target])\n",
        "  ret_l, ret_r, ret_target = shuffle(ret_l, ret_r, ret_target)\n",
        "\n",
        "  return ret_l, ret_r, ret_target\n",
        "\n",
        "tr_l, tr_r, tr_target = pre(train_data)\n",
        "te_l, te_r, te_target = pre(test_data)\n",
        "\n",
        "\n",
        "print('left train data length:', len(tr_l))\n",
        "print('left test data length:', len(te_l))\n",
        "\n"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "left train data length: 400\n",
            "left test data length: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iLERZXbBJ7aG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# model building\n"
      ]
    },
    {
      "metadata": {
        "id": "wyOnY-1g7cSm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1D-CNN"
      ]
    },
    {
      "metadata": {
        "id": "sPdQDp-3XJ4J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "dim_dense = 32\n",
        "\n",
        "dropout_rate = 0.2\n",
        "regularizer_coef = 0.01\n",
        "rolling_window = 60\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ah6nTA1wUbFY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Input, Dense, Lambda\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "from keras import Model\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wJXwcO9k1TQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "0da02f7a-af4d-4795-d3db-55e47884b551"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "input_l = Input(shape=(max_len, len(uni_words)), name = 'input_layer_l')\n",
        "input_r = Input(shape=(max_len, len(uni_words)), name = 'input_layer_r')\n",
        "\n",
        "conv_1_l = Conv1D(256, kernel_size=2, kernel_regularizer=l2(regularizer_coef), name=\"conv_1_l\")(input_l)\n",
        "conv_1_r = Conv1D(256, kernel_size=2, kernel_regularizer=l2(regularizer_coef), name=\"conv_1_r\")(input_r)\n",
        "\n",
        "pool_1_l = MaxPooling1D(pool_size=2, name=\"pool_1_l\")(conv_1_l)\n",
        "pool_1_r= MaxPooling1D(pool_size=2, name=\"pool_1_r\")(conv_1_r)\n",
        "\n",
        "drop_1_l = Dropout(dropout_rate)(pool_1_l)\n",
        "drop_1_r = Dropout(dropout_rate)(pool_1_r)\n",
        "\n",
        "\n",
        "conv_2_l = Conv1D(128, kernel_size=2, kernel_regularizer=l2(regularizer_coef), name=\"conv_3_l\")(drop_1_l)\n",
        "conv_2_r = Conv1D(128, kernel_size=2, kernel_regularizer=l2(regularizer_coef), name=\"conv_3_r\")(drop_1_r)\n",
        "\n",
        "pool_2_l = MaxPooling1D(pool_size=2, name=\"pool_4_l\")(conv_2_l)\n",
        "pool_2_r= MaxPooling1D(pool_size=2, name=\"pool_4_r\")(conv_2_r)\n",
        "\n",
        "drop_2_l = Dropout(dropout_rate)(pool_2_l)\n",
        "drop_2_r = Dropout(dropout_rate)(pool_2_r)\n",
        "\n",
        "# conv_3_l = Conv1D(32, kernel_size=2, kernel_regularizer=l2(regularizer_coef), name=\"conv_5_l\")(drop_2_l)\n",
        "# conv_3_r = Conv1D(32, kernel_size=2, kernel_regularizer=l2(regularizer_coef), name=\"conv_5_r\")(drop_2_r)\n",
        "\n",
        "# pool_3_l = MaxPooling1D(pool_size=2, name=\"pool_6_l\")(conv_3_l)\n",
        "# pool_3_r= MaxPooling1D(pool_size=2, name=\"pool_6_r\")(conv_3_r)\n",
        "\n",
        "# drop_3_l = Dropout(dropout_rate)(pool_3_l)\n",
        "# drop_3_r = Dropout(dropout_rate)(pool_3_r)\n",
        "\n",
        "\n",
        "flat_l = Flatten(name=\"flat_l\")(drop_2_l)\n",
        "flat_r = Flatten(name=\"flat_r\")(drop_2_r)\n",
        "\n",
        "dense_7_l = Dense(dim_dense, name=\"dense_7_l\")(flat_l)\n",
        "dense_7_r = Dense(dim_dense, name=\"dense_7_r\")(flat_r)\n",
        "\n",
        "\n",
        "\n",
        "L1_distance = lambda x: K.abs(x[0]-x[1])\n",
        "\n",
        "both = Lambda(L1_distance, output_shape=lambda x: x[0])([dense_7_l, dense_7_r])\n",
        "# both = merge([flat1, flat2], mode=L1_distance, output_shape=lambda x: x[0])\n",
        "\n",
        "prediction = Dense(1,activation='sigmoid', name=\"output\")(both)\n",
        "\n",
        "\n",
        "model = Model([input_l, input_r], prediction)\n",
        "model.summary()"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_layer_l (InputLayer)      (None, 18, 433)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_layer_r (InputLayer)      (None, 18, 433)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_1_l (Conv1D)               (None, 17, 256)      221952      input_layer_l[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_1_r (Conv1D)               (None, 17, 256)      221952      input_layer_r[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "pool_1_l (MaxPooling1D)         (None, 8, 256)       0           conv_1_l[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool_1_r (MaxPooling1D)         (None, 8, 256)       0           conv_1_r[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 8, 256)       0           pool_1_l[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 8, 256)       0           pool_1_r[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv_3_l (Conv1D)               (None, 7, 128)       65664       dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_3_r (Conv1D)               (None, 7, 128)       65664       dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool_4_l (MaxPooling1D)         (None, 3, 128)       0           conv_3_l[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool_4_r (MaxPooling1D)         (None, 3, 128)       0           conv_3_r[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 3, 128)       0           pool_4_l[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 3, 128)       0           pool_4_r[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flat_l (Flatten)                (None, 384)          0           dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flat_r (Flatten)                (None, 384)          0           dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_7_l (Dense)               (None, 32)           12320       flat_l[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_7_r (Dense)               (None, 32)           12320       flat_r[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 32)           0           dense_7_l[0][0]                  \n",
            "                                                                 dense_7_r[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 1)            33          lambda_11[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 599,905\n",
            "Trainable params: 599,905\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jwosaMM-azYK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for i in range(30):\n",
        "#   print(\"=========> epoch:\", i)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "num_epoches = 30\n",
        "\n",
        "val_prop = 0.8\n",
        "\n",
        "learning_rate = 0.005\n",
        "\n",
        "val_pos = int(len(tr_l) * val_prop)\n",
        "\n",
        "ttr_l = tr_l[val_pos:]\n",
        "ttr_r = tr_r[val_pos:]\n",
        "ttr_target = tr_target[val_pos:]\n",
        "\n",
        "val_l = tr_l[:val_pos]\n",
        "val_r = tr_r[:val_pos]\n",
        "val_target = tr_target[:val_pos]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z7zguUlV7bNt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizers.Adam(learning_rate), loss=\"binary_crossentropy\", metrics=['acc'])\n",
        "\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3, \\\n",
        "                          verbose=1, mode='auto')\n",
        "callbacks_list = [earlystop]\n",
        "model.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HOUVswp00pcx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c471afe4-f694-4a0d-9833-288feaf73793"
      },
      "cell_type": "code",
      "source": [
        "!ps -aux"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n",
            "root         1  0.0  0.0  39144  5152 ?        Ss   07:39   0:00 /bin/bash -e /d\r\n",
            "root        69  0.0  0.2 678296 29408 ?        Sl   07:40   0:05 node /tools/nod\r\n",
            "root        79  0.2  0.3 723424 50596 ?        Sl   07:40   0:46 /tools/node/bin\r\n",
            "root        89  0.2  0.4 213128 60752 ?        Sl   07:40   0:41 /usr/bin/python\r\n",
            "root       435 43.2 51.8 46249004 6911308 ?    Ssl  12:08  17:36 /usr/bin/python\r\n",
            "root      1246  113  0.0  33960  4772 pts/0    Ss+  12:49   0:01 /bin/sh -c ps -\r\n",
            "root      1247  0.0  0.0  63304  6628 pts/0    R+   12:49   0:00 ps -aux\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3lr98VDpB3nh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3451
        },
        "outputId": "cc9af82d-f778-497a-8024-fe1918e49968"
      },
      "cell_type": "code",
      "source": [
        "model.fit([train_data[tr_l],train_data[tr_r]], tr_target, epochs= 100, validation_split=0.2)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 320 samples, validate on 80 samples\n",
            "Epoch 1/100\n",
            "320/320 [==============================] - 3s 9ms/step - loss: 5.8960 - acc: 0.4719 - val_loss: 2.1663 - val_acc: 0.4125\n",
            "Epoch 2/100\n",
            "320/320 [==============================] - 0s 658us/step - loss: 1.5161 - acc: 0.4688 - val_loss: 1.2022 - val_acc: 0.4750\n",
            "Epoch 3/100\n",
            "320/320 [==============================] - 0s 665us/step - loss: 1.1023 - acc: 0.4906 - val_loss: 0.9944 - val_acc: 0.4750\n",
            "Epoch 4/100\n",
            "320/320 [==============================] - 0s 648us/step - loss: 0.8841 - acc: 0.4719 - val_loss: 0.8141 - val_acc: 0.3625\n",
            "Epoch 5/100\n",
            "320/320 [==============================] - 0s 661us/step - loss: 0.7859 - acc: 0.5250 - val_loss: 0.7575 - val_acc: 0.4750\n",
            "Epoch 6/100\n",
            "320/320 [==============================] - 0s 657us/step - loss: 0.7392 - acc: 0.4906 - val_loss: 0.7274 - val_acc: 0.4750\n",
            "Epoch 7/100\n",
            "320/320 [==============================] - 0s 674us/step - loss: 0.7210 - acc: 0.4469 - val_loss: 0.7140 - val_acc: 0.4750\n",
            "Epoch 8/100\n",
            "320/320 [==============================] - 0s 632us/step - loss: 0.7169 - acc: 0.5000 - val_loss: 0.7057 - val_acc: 0.5250\n",
            "Epoch 9/100\n",
            "320/320 [==============================] - 0s 642us/step - loss: 0.7190 - acc: 0.4750 - val_loss: 0.7116 - val_acc: 0.4750\n",
            "Epoch 10/100\n",
            "320/320 [==============================] - 0s 666us/step - loss: 0.7044 - acc: 0.5031 - val_loss: 0.6993 - val_acc: 0.5250\n",
            "Epoch 11/100\n",
            "320/320 [==============================] - 0s 651us/step - loss: 0.7023 - acc: 0.4906 - val_loss: 0.7088 - val_acc: 0.4750\n",
            "Epoch 12/100\n",
            "320/320 [==============================] - 0s 629us/step - loss: 0.7017 - acc: 0.4844 - val_loss: 0.6974 - val_acc: 0.4125\n",
            "Epoch 13/100\n",
            "320/320 [==============================] - 0s 645us/step - loss: 0.6972 - acc: 0.5062 - val_loss: 0.6988 - val_acc: 0.4750\n",
            "Epoch 14/100\n",
            "320/320 [==============================] - 0s 669us/step - loss: 0.7023 - acc: 0.4469 - val_loss: 0.7012 - val_acc: 0.4750\n",
            "Epoch 15/100\n",
            "320/320 [==============================] - 0s 640us/step - loss: 0.6981 - acc: 0.5156 - val_loss: 0.6945 - val_acc: 0.5250\n",
            "Epoch 16/100\n",
            "320/320 [==============================] - 0s 675us/step - loss: 0.6991 - acc: 0.4906 - val_loss: 0.7005 - val_acc: 0.4750\n",
            "Epoch 17/100\n",
            "320/320 [==============================] - 0s 666us/step - loss: 0.6953 - acc: 0.5094 - val_loss: 0.6946 - val_acc: 0.4750\n",
            "Epoch 18/100\n",
            "320/320 [==============================] - 0s 667us/step - loss: 0.6962 - acc: 0.4813 - val_loss: 0.6946 - val_acc: 0.4750\n",
            "Epoch 19/100\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.6919 - acc: 0.5156"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "320/320 [==============================] - 0s 655us/step - loss: 0.6972 - acc: 0.5000 - val_loss: 0.6933 - val_acc: 0.5250\n",
            "Epoch 20/100\n",
            "320/320 [==============================] - 0s 663us/step - loss: 0.7293 - acc: 0.4500 - val_loss: 0.7268 - val_acc: 0.4750\n",
            "Epoch 21/100\n",
            "320/320 [==============================] - 0s 679us/step - loss: 0.6972 - acc: 0.5094 - val_loss: 0.6931 - val_acc: 0.5250\n",
            "Epoch 22/100\n",
            "320/320 [==============================] - 0s 614us/step - loss: 0.6996 - acc: 0.4594 - val_loss: 0.6941 - val_acc: 0.5250\n",
            "Epoch 23/100\n",
            "320/320 [==============================] - 0s 648us/step - loss: 0.6965 - acc: 0.4875 - val_loss: 0.7170 - val_acc: 0.4750\n",
            "Epoch 24/100\n",
            "320/320 [==============================] - 0s 631us/step - loss: 0.7113 - acc: 0.4531 - val_loss: 0.6999 - val_acc: 0.4750\n",
            "Epoch 25/100\n",
            "320/320 [==============================] - 0s 652us/step - loss: 0.7066 - acc: 0.5125 - val_loss: 0.7005 - val_acc: 0.4750\n",
            "Epoch 26/100\n",
            "320/320 [==============================] - 0s 656us/step - loss: 0.7038 - acc: 0.4844 - val_loss: 0.6980 - val_acc: 0.5250\n",
            "Epoch 27/100\n",
            "320/320 [==============================] - 0s 667us/step - loss: 0.6997 - acc: 0.4875 - val_loss: 0.7014 - val_acc: 0.4750\n",
            "Epoch 28/100\n",
            "320/320 [==============================] - 0s 653us/step - loss: 0.7015 - acc: 0.5125 - val_loss: 0.6997 - val_acc: 0.4750\n",
            "Epoch 29/100\n",
            "320/320 [==============================] - 0s 676us/step - loss: 0.6982 - acc: 0.5031 - val_loss: 0.6972 - val_acc: 0.4750\n",
            "Epoch 30/100\n",
            "320/320 [==============================] - 0s 660us/step - loss: 0.6980 - acc: 0.5000 - val_loss: 0.6978 - val_acc: 0.4750\n",
            "Epoch 31/100\n",
            "320/320 [==============================] - 0s 663us/step - loss: 0.6977 - acc: 0.4938 - val_loss: 0.6965 - val_acc: 0.4750\n",
            "Epoch 32/100\n",
            "320/320 [==============================] - 0s 661us/step - loss: 0.6984 - acc: 0.4906 - val_loss: 0.6932 - val_acc: 0.5250\n",
            "Epoch 33/100\n",
            "320/320 [==============================] - 0s 625us/step - loss: 0.6982 - acc: 0.4969 - val_loss: 0.6979 - val_acc: 0.4750\n",
            "Epoch 34/100\n",
            "320/320 [==============================] - 0s 619us/step - loss: 0.6985 - acc: 0.5125 - val_loss: 0.6992 - val_acc: 0.4750\n",
            "Epoch 35/100\n",
            "320/320 [==============================] - 0s 653us/step - loss: 0.6941 - acc: 0.5062 - val_loss: 0.6932 - val_acc: 0.5250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36/100\n",
            "320/320 [==============================] - 0s 657us/step - loss: 0.6970 - acc: 0.4688 - val_loss: 0.6939 - val_acc: 0.5250\n",
            "Epoch 37/100\n",
            "320/320 [==============================] - 0s 655us/step - loss: 0.7000 - acc: 0.4719 - val_loss: 0.6989 - val_acc: 0.4750\n",
            "Epoch 38/100\n",
            "320/320 [==============================] - 0s 660us/step - loss: 0.6988 - acc: 0.5125 - val_loss: 0.7007 - val_acc: 0.4750\n",
            "Epoch 39/100\n",
            "320/320 [==============================] - 0s 669us/step - loss: 0.6987 - acc: 0.4906 - val_loss: 0.7076 - val_acc: 0.4750\n",
            "Epoch 40/100\n",
            "320/320 [==============================] - 0s 643us/step - loss: 0.7011 - acc: 0.5000 - val_loss: 0.6977 - val_acc: 0.5250\n",
            "Epoch 41/100\n",
            "320/320 [==============================] - 0s 651us/step - loss: 0.7098 - acc: 0.4719 - val_loss: 0.7035 - val_acc: 0.4750\n",
            "Epoch 42/100\n",
            "320/320 [==============================] - 0s 655us/step - loss: 0.7058 - acc: 0.4313 - val_loss: 0.7007 - val_acc: 0.5250\n",
            "Epoch 43/100\n",
            "320/320 [==============================] - 0s 665us/step - loss: 0.7014 - acc: 0.4562 - val_loss: 0.7020 - val_acc: 0.4750\n",
            "Epoch 44/100\n",
            "320/320 [==============================] - 0s 659us/step - loss: 0.7042 - acc: 0.5094 - val_loss: 0.7014 - val_acc: 0.4750\n",
            "Epoch 45/100\n",
            "320/320 [==============================] - 0s 671us/step - loss: 0.6977 - acc: 0.5156 - val_loss: 0.6961 - val_acc: 0.5250\n",
            "Epoch 46/100\n",
            "320/320 [==============================] - 0s 671us/step - loss: 0.6996 - acc: 0.4844 - val_loss: 0.6991 - val_acc: 0.4750\n",
            "Epoch 47/100\n",
            "320/320 [==============================] - 0s 667us/step - loss: 0.6981 - acc: 0.5156 - val_loss: 0.6986 - val_acc: 0.4750\n",
            "Epoch 48/100\n",
            "320/320 [==============================] - 0s 658us/step - loss: 0.6980 - acc: 0.4781 - val_loss: 0.6956 - val_acc: 0.5250\n",
            "Epoch 49/100\n",
            "320/320 [==============================] - 0s 682us/step - loss: 0.6985 - acc: 0.4844 - val_loss: 0.6989 - val_acc: 0.4750\n",
            "Epoch 50/100\n",
            "320/320 [==============================] - 0s 660us/step - loss: 0.7014 - acc: 0.5156 - val_loss: 0.7019 - val_acc: 0.4750\n",
            "Epoch 51/100\n",
            "320/320 [==============================] - 0s 636us/step - loss: 0.7031 - acc: 0.4938 - val_loss: 0.7084 - val_acc: 0.4750\n",
            "Epoch 52/100\n",
            "320/320 [==============================] - 0s 645us/step - loss: 0.7266 - acc: 0.4875 - val_loss: 0.7276 - val_acc: 0.4750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 53/100\n",
            "320/320 [==============================] - 0s 634us/step - loss: 0.7239 - acc: 0.5125 - val_loss: 0.7162 - val_acc: 0.4750\n",
            "Epoch 54/100\n",
            "320/320 [==============================] - 0s 644us/step - loss: 0.7170 - acc: 0.4844 - val_loss: 0.7287 - val_acc: 0.4750\n",
            "Epoch 55/100\n",
            "320/320 [==============================] - 0s 649us/step - loss: 0.7191 - acc: 0.5062 - val_loss: 0.7165 - val_acc: 0.4750\n",
            "Epoch 56/100\n",
            "320/320 [==============================] - 0s 641us/step - loss: 0.7127 - acc: 0.5125 - val_loss: 0.7071 - val_acc: 0.4750\n",
            "Epoch 57/100\n",
            "320/320 [==============================] - 0s 664us/step - loss: 0.7067 - acc: 0.5187 - val_loss: 0.7011 - val_acc: 0.5250\n",
            "Epoch 58/100\n",
            "320/320 [==============================] - 0s 643us/step - loss: 0.7070 - acc: 0.4875 - val_loss: 0.7012 - val_acc: 0.4750\n",
            "Epoch 59/100\n",
            "320/320 [==============================] - 0s 664us/step - loss: 0.7015 - acc: 0.4938 - val_loss: 0.7045 - val_acc: 0.4750\n",
            "Epoch 60/100\n",
            "320/320 [==============================] - 0s 637us/step - loss: 0.7015 - acc: 0.4906 - val_loss: 0.6973 - val_acc: 0.4500\n",
            "Epoch 61/100\n",
            "320/320 [==============================] - 0s 664us/step - loss: 0.6970 - acc: 0.4969 - val_loss: 0.6997 - val_acc: 0.4750\n",
            "Epoch 62/100\n",
            "320/320 [==============================] - 0s 616us/step - loss: 0.6963 - acc: 0.5125 - val_loss: 0.6974 - val_acc: 0.4750\n",
            "Epoch 63/100\n",
            "320/320 [==============================] - 0s 633us/step - loss: 0.6959 - acc: 0.5125 - val_loss: 0.6964 - val_acc: 0.4750\n",
            "Epoch 64/100\n",
            "320/320 [==============================] - 0s 626us/step - loss: 0.6937 - acc: 0.5031 - val_loss: 0.6944 - val_acc: 0.4750\n",
            "Epoch 65/100\n",
            "320/320 [==============================] - 0s 639us/step - loss: 0.6978 - acc: 0.5156 - val_loss: 0.6973 - val_acc: 0.4750\n",
            "Epoch 66/100\n",
            "320/320 [==============================] - 0s 618us/step - loss: 0.6976 - acc: 0.4500 - val_loss: 0.6933 - val_acc: 0.5250\n",
            "Epoch 67/100\n",
            "320/320 [==============================] - 0s 651us/step - loss: 0.6954 - acc: 0.4719 - val_loss: 0.6949 - val_acc: 0.4750\n",
            "Epoch 68/100\n",
            "320/320 [==============================] - 0s 658us/step - loss: 0.6938 - acc: 0.5125 - val_loss: 0.6956 - val_acc: 0.4750\n",
            "Epoch 69/100\n",
            "320/320 [==============================] - 0s 660us/step - loss: 0.6938 - acc: 0.5125 - val_loss: 0.6953 - val_acc: 0.4750\n",
            "Epoch 70/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "320/320 [==============================] - 0s 664us/step - loss: 0.6936 - acc: 0.5125 - val_loss: 0.6950 - val_acc: 0.4750\n",
            "Epoch 71/100\n",
            "320/320 [==============================] - 0s 661us/step - loss: 0.6935 - acc: 0.5125 - val_loss: 0.6957 - val_acc: 0.4750\n",
            "Epoch 72/100\n",
            "320/320 [==============================] - 0s 652us/step - loss: 0.6947 - acc: 0.5125 - val_loss: 0.6967 - val_acc: 0.4750\n",
            "Epoch 73/100\n",
            "320/320 [==============================] - 0s 668us/step - loss: 0.6951 - acc: 0.4688 - val_loss: 0.6934 - val_acc: 0.5250\n",
            "Epoch 74/100\n",
            "320/320 [==============================] - 0s 645us/step - loss: 0.6939 - acc: 0.4906 - val_loss: 0.6942 - val_acc: 0.4750\n",
            "Epoch 75/100\n",
            "320/320 [==============================] - 0s 695us/step - loss: 0.6930 - acc: 0.5187 - val_loss: 0.6949 - val_acc: 0.4750\n",
            "Epoch 76/100\n",
            "320/320 [==============================] - 0s 639us/step - loss: 0.6942 - acc: 0.5125 - val_loss: 0.6950 - val_acc: 0.4750\n",
            "Epoch 77/100\n",
            "320/320 [==============================] - 0s 641us/step - loss: 0.6946 - acc: 0.5125 - val_loss: 0.6953 - val_acc: 0.4750\n",
            "Epoch 78/100\n",
            "320/320 [==============================] - 0s 667us/step - loss: 0.6935 - acc: 0.5125 - val_loss: 0.6947 - val_acc: 0.4750\n",
            "Epoch 79/100\n",
            "320/320 [==============================] - 0s 664us/step - loss: 0.6941 - acc: 0.4531 - val_loss: 0.6937 - val_acc: 0.4750\n",
            "Epoch 80/100\n",
            "320/320 [==============================] - 0s 653us/step - loss: 0.6934 - acc: 0.5094 - val_loss: 0.6943 - val_acc: 0.4750\n",
            "Epoch 81/100\n",
            "320/320 [==============================] - 0s 665us/step - loss: 0.6949 - acc: 0.4750 - val_loss: 0.6957 - val_acc: 0.4750\n",
            "Epoch 82/100\n",
            "320/320 [==============================] - 0s 624us/step - loss: 0.6943 - acc: 0.5125 - val_loss: 0.6978 - val_acc: 0.4750\n",
            "Epoch 83/100\n",
            "320/320 [==============================] - 0s 659us/step - loss: 0.6947 - acc: 0.5062 - val_loss: 0.6941 - val_acc: 0.4750\n",
            "Epoch 84/100\n",
            "320/320 [==============================] - 0s 667us/step - loss: 0.6945 - acc: 0.4688 - val_loss: 0.6946 - val_acc: 0.4750\n",
            "Epoch 85/100\n",
            "320/320 [==============================] - 0s 632us/step - loss: 0.6941 - acc: 0.5125 - val_loss: 0.6979 - val_acc: 0.4750\n",
            "Epoch 86/100\n",
            "320/320 [==============================] - 0s 634us/step - loss: 0.6941 - acc: 0.5125 - val_loss: 0.6939 - val_acc: 0.4750\n",
            "Epoch 87/100\n",
            "320/320 [==============================] - 0s 660us/step - loss: 0.6964 - acc: 0.5062 - val_loss: 0.6965 - val_acc: 0.4750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 88/100\n",
            "320/320 [==============================] - 0s 666us/step - loss: 0.6940 - acc: 0.4906 - val_loss: 0.6925 - val_acc: 0.5250\n",
            "Epoch 89/100\n",
            "320/320 [==============================] - 0s 652us/step - loss: 0.6942 - acc: 0.4906 - val_loss: 0.6933 - val_acc: 0.4750\n",
            "Epoch 90/100\n",
            "320/320 [==============================] - 0s 660us/step - loss: 0.6927 - acc: 0.5250 - val_loss: 0.6953 - val_acc: 0.4750\n",
            "Epoch 91/100\n",
            "320/320 [==============================] - 0s 637us/step - loss: 0.6931 - acc: 0.5125 - val_loss: 0.6967 - val_acc: 0.4750\n",
            "Epoch 92/100\n",
            "320/320 [==============================] - 0s 643us/step - loss: 0.6932 - acc: 0.5125 - val_loss: 0.6949 - val_acc: 0.4750\n",
            "Epoch 93/100\n",
            "320/320 [==============================] - 0s 632us/step - loss: 0.6933 - acc: 0.5125 - val_loss: 0.6942 - val_acc: 0.4750\n",
            "Epoch 94/100\n",
            "320/320 [==============================] - 0s 652us/step - loss: 0.6932 - acc: 0.5125 - val_loss: 0.6951 - val_acc: 0.4750\n",
            "Epoch 95/100\n",
            "320/320 [==============================] - 0s 645us/step - loss: 0.6939 - acc: 0.5094 - val_loss: 0.6936 - val_acc: 0.4750\n",
            "Epoch 96/100\n",
            "320/320 [==============================] - 0s 660us/step - loss: 0.6948 - acc: 0.4594 - val_loss: 0.6930 - val_acc: 0.5250\n",
            "Epoch 97/100\n",
            "320/320 [==============================] - 0s 651us/step - loss: 0.6942 - acc: 0.4844 - val_loss: 0.6964 - val_acc: 0.4750\n",
            "Epoch 98/100\n",
            "320/320 [==============================] - 0s 652us/step - loss: 0.6947 - acc: 0.5125 - val_loss: 0.6941 - val_acc: 0.4750\n",
            "Epoch 99/100\n",
            "320/320 [==============================] - 0s 665us/step - loss: 0.6928 - acc: 0.5125 - val_loss: 0.6964 - val_acc: 0.4750\n",
            "Epoch 100/100\n",
            "320/320 [==============================] - 0s 651us/step - loss: 0.6932 - acc: 0.5125 - val_loss: 0.6950 - val_acc: 0.4750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0d39388ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "metadata": {
        "id": "34Z5AP8r4OIZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1547
        },
        "outputId": "e454b697-5beb-4f45-fdfd-06ccae0198cd"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "for n in range(num_epoches):\n",
        "\n",
        "  print('epoch %d of %d' % (n+1, num_epoches))\n",
        "\n",
        "  num_iteration = len(ttr_l)//batch_size\n",
        "  for i in range(num_iteration):\n",
        "    s = i*batch_size\n",
        "    e = (i+1)*batch_size\n",
        "    tr_loss = model.train_on_batch([train_data[ttr_l[s:e]],train_data[ttr_r[s:e]]], ttr_target[s:e])\n",
        "  validation = model.evaluate([train_data[val_l], train_data[val_r]], val_target)\n",
        "  print(\"  train loss:\",tr_loss ,\"evaluation:\", validation)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1 of 30\n",
            "160/160 [==============================] - 0s 2ms/step\n",
            "  train loss: [2.9539633, 0.5] evaluation: [2.728298044204712, 0.4875]\n",
            "epoch 2 of 30\n",
            "160/160 [==============================] - 0s 205us/step\n",
            "  train loss: [1.7108607, 0.53125] evaluation: [1.6610628843307496, 0.4875]\n",
            "epoch 3 of 30\n",
            "160/160 [==============================] - 0s 190us/step\n",
            "  train loss: [1.0533506, 0.53125] evaluation: [1.0395971536636353, 0.54375]\n",
            "epoch 4 of 30\n",
            "160/160 [==============================] - 0s 218us/step\n",
            "  train loss: [0.8822906, 0.46875] evaluation: [0.8814955115318298, 0.4875]\n",
            "epoch 5 of 30\n",
            "160/160 [==============================] - 0s 209us/step\n",
            "  train loss: [0.83635944, 0.53125] evaluation: [0.8320917129516602, 0.4875]\n",
            "epoch 6 of 30\n",
            "160/160 [==============================] - 0s 237us/step\n",
            "  train loss: [0.76853216, 0.53125] evaluation: [0.7733056426048279, 0.4875]\n",
            "epoch 7 of 30\n",
            "160/160 [==============================] - 0s 224us/step\n",
            "  train loss: [0.76467794, 0.46875] evaluation: [0.7558952450752259, 0.5125]\n",
            "epoch 8 of 30\n",
            "160/160 [==============================] - 0s 217us/step\n",
            "  train loss: [0.73190475, 0.53125] evaluation: [0.7374469995498657, 0.4875]\n",
            "epoch 9 of 30\n",
            "160/160 [==============================] - 0s 264us/step\n",
            "  train loss: [0.72487193, 0.40625] evaluation: [0.7234489440917968, 0.5125]\n",
            "epoch 10 of 30\n",
            "160/160 [==============================] - 0s 240us/step\n",
            "  train loss: [0.73270756, 0.46875] evaluation: [0.7252493858337402, 0.5125]\n",
            "epoch 11 of 30\n",
            "160/160 [==============================] - 0s 233us/step\n",
            "  train loss: [0.7110504, 0.53125] evaluation: [0.7102166533470153, 0.4875]\n",
            "epoch 12 of 30\n",
            "160/160 [==============================] - 0s 255us/step\n",
            "  train loss: [0.71097213, 0.53125] evaluation: [0.710710096359253, 0.5125]\n",
            "epoch 13 of 30\n",
            "160/160 [==============================] - 0s 225us/step\n",
            "  train loss: [0.71071476, 0.53125] evaluation: [0.7098950386047364, 0.4875]\n",
            "epoch 14 of 30\n",
            "160/160 [==============================] - 0s 216us/step\n",
            "  train loss: [0.70263743, 0.53125] evaluation: [0.7040628433227539, 0.4875]\n",
            "epoch 15 of 30\n",
            "160/160 [==============================] - 0s 223us/step\n",
            "  train loss: [0.7083021, 0.53125] evaluation: [0.7076626420021057, 0.5125]\n",
            "epoch 16 of 30\n",
            "160/160 [==============================] - 0s 225us/step\n",
            "  train loss: [0.69965833, 0.53125] evaluation: [0.7031980991363526, 0.4875]\n",
            "epoch 17 of 30\n",
            "160/160 [==============================] - 0s 232us/step\n",
            "  train loss: [0.6964274, 0.53125] evaluation: [0.6991090178489685, 0.4875]\n",
            "epoch 18 of 30\n",
            "160/160 [==============================] - 0s 270us/step\n",
            "  train loss: [0.698436, 0.53125] evaluation: [0.6995903968811035, 0.4875]\n",
            "epoch 19 of 30\n",
            "160/160 [==============================] - 0s 226us/step\n",
            "  train loss: [0.69840956, 0.53125] evaluation: [0.6986165881156922, 0.4875]\n",
            "epoch 20 of 30\n",
            "160/160 [==============================] - 0s 226us/step\n",
            "  train loss: [0.69819105, 0.53125] evaluation: [0.6986825466156006, 0.4875]\n",
            "epoch 21 of 30\n",
            "160/160 [==============================] - 0s 222us/step\n",
            "  train loss: [0.70268327, 0.625] evaluation: [0.704337728023529, 0.4875]\n",
            "epoch 22 of 30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "160/160 [==============================] - 0s 264us/step\n",
            "  train loss: [0.71099997, 0.53125] evaluation: [0.7155989050865174, 0.4875]\n",
            "epoch 23 of 30\n",
            "160/160 [==============================] - 0s 223us/step\n",
            "  train loss: [0.7310233, 0.53125] evaluation: [0.7353568792343139, 0.4875]\n",
            "epoch 24 of 30\n",
            "160/160 [==============================] - 0s 211us/step\n",
            "  train loss: [0.700984, 0.53125] evaluation: [0.7037813305854798, 0.4875]\n",
            "epoch 25 of 30\n",
            "160/160 [==============================] - 0s 240us/step\n",
            "  train loss: [0.6961221, 0.5625] evaluation: [0.6962440371513366, 0.4875]\n",
            "epoch 26 of 30\n",
            "160/160 [==============================] - 0s 234us/step\n",
            "  train loss: [0.6960712, 0.53125] evaluation: [0.695504879951477, 0.4875]\n",
            "epoch 27 of 30\n",
            "160/160 [==============================] - 0s 217us/step\n",
            "  train loss: [0.6944392, 0.53125] evaluation: [0.6949926376342773, 0.4875]\n",
            "epoch 28 of 30\n",
            "160/160 [==============================] - 0s 261us/step\n",
            "  train loss: [0.6961707, 0.53125] evaluation: [0.6947717308998108, 0.4875]\n",
            "epoch 29 of 30\n",
            "160/160 [==============================] - 0s 221us/step\n",
            "  train loss: [0.6950692, 0.53125] evaluation: [0.6944765448570251, 0.4875]\n",
            "epoch 30 of 30\n",
            "160/160 [==============================] - 0s 243us/step\n",
            "  train loss: [0.69557214, 0.53125] evaluation: [0.6971071124076843, 0.4875]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7ukHeucsHOHP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "When the data set is too large, it will introduce too many dimensions, which will lead to the explotion of the error rate. The solution might be the Word2Vec\n"
      ]
    },
    {
      "metadata": {
        "id": "a2UbAN8gUki6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}